<!DOCTYPE html> 
<html> 
<head> <title></title> 
<meta charset='UTF-8' /> 
<meta content='TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)' name='generator' /> 
<link href='main.css' rel='stylesheet' type='text/css' /> 
<!--  for beautifying  --><link href='site.css' rel='stylesheet' type='text/css' /> 
<script type='text/x-mathjax-config'> MathJax.Hub.Config({ extensions: ["tex2jax.js"], jax: ["input/TeX", "output/HTML-CSS"], tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, "HTML-CSS": { availableFonts: ["TeX"] } }); </script> <script src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' type='text/javascript'></script> 
</head><body>
<p>Footer <span class='tcrm-1000'>© </span>2023 GitHub, Inc.
   </p> 
  <div class='maketitle'>                                                              

                                                                  
                                                                  

                                                                  

<h2 class='titleHead'> You Only Look Once... Unless It’s Blurry<br />
From Blurry Pixels to Causality: Building Real-Time Vision Apps </h2>
 <div class='author'><span class='cmr-12'>B1ngster</span></div><br />
<div class='date'><span class='cmr-12'>31 July 2025</span></div>
   </div>
                                                                  

                                                                  
   
   <h3 class='likesectionHead' id='seeing-isnt-always-understanding'><a id='x1-1000'></a>Seeing Isn’t Always Understanding</h3>
<p id='seeing-isnt-always-understanding1'><a id='Q1-1-2'></a>
</p><p>Ever since Donald Trump became the leader of the free world, its now become
possible to achieve anything you want. Not that Donald Trump has achieved things
himself - he is nepo baby who was funded by another nepo baby who becoming
the head of a Government Department without being elected and he is not
even American. Now, that person is no longer leading the department, and
Donald Trump might no longer be president. A lot has happened in American
politics since my last article, but we’re not going to talk about that right
now... On June 2nd, Prime Minister Keir Starmer announced that every
British citizen should be prepared to go to war; perhaps we’ll talk about that
instead...
</p> 
<p>   Anyway, I’ve recently felt inspired to explore augmented reality—something that’s
become possible now that I’ve been approved for a mobile contract. A friend recently
asked me if I could help identify some numbers in a video file, specifically the collar
number of a police officer.
</p> 
<p>   I was optimistic that this would be a great opportunity to apply some data
science skills. After loading the video into <code><span class='cmtt-10'>Blender</span></code>, I was able to identify most of the
numbers across different frames. However, the footage was blurry, and I couldn’t be
certain about one of the digits.
</p> 
<p>   This issue also highlights a limitation I’ve encountered with my phone. The
Samsung S24’s accessibility tool,  <a href='https://www.samsung.com'><span class='underline'>Bixby Vision</span></a>, offers only basic OCR—it captures
text from photos, but doesn’t allow direct text selection or copying, and
images often turn out partially out of focus. There are clearly more advanced
apps available with richer scene understanding and better OCR accuracy.
This issue also highlights a limitation I’ve encountered with my phone. The
Samsung S24’s accessibility tool,  <a href='https://www.samsung.com'><span class='underline'>Bixby Vision</span></a>, offers only basic OCR—it
captures text from photos, but doesn’t allow direct text selection or copying,
and images often turn out partially out of focus. There are clearly more
advanced apps available with richer scene understanding and better OCR
accuracy.
</p> 
<p>    <a href='https://developers.google.com/ml-kit/overview'><span class='underline'>Google’s ML Kit</span></a> includes a Scene Semantics API capable of identifying
buildings, vehicles, people, and other common objects. It also provides a document
scanner API, as well as several natural language processing APIs such as entity
extraction and pose detection—including depth-aware pose estimation using Z
coordinates. ML Kit supports the integration of TensorFlow models, allowing
developers to enhance object recognition with custom AI.
</p> 
<p>   <a href='https://ai.google'><span class='underline'>Gemini</span></a> is available in ARCore, but it is currently limited to Pixel phones.
Like ChatGPT, <code><span class='cmtt-10'>Gemini</span></code> is a transformer model that can understand images,
though video understanding hasn’t been implemented yet. Unfortunately,
many of these APIs come with usage costs, while ARCore appears to be free.
It’s possible that these third-party services could see our use as potentially
                                                                  

                                                                  
nefarious.
   
   </p> 

   <h3 class='likesectionHead' id='beyond-boxes-yolo-detr-and-the-leap-toward-causality'><a id='x1-2000'></a>Beyond Boxes: YOLO, DETR, and the Leap Toward Causality</h3>
<p id='beyond-boxes-yolo-detr-and-the-leap-toward-causality1'><a id='Q1-1-4'></a>
</p><p>There are several video transformer models available—something I’m quite familiar
with, as they were among the potential topics for my master’s dissertation. In the
end, I chose to focus on identifying faults in the electric grid—a subject that may
be worth revisiting, especially now that I’ve received a Carbon Literacy
certificate.
</p> 
<p>   Drones are often built using the <code><span class='cmtt-10'>Raspberry Pi</span></code>, developed by a British company
that manufactures most of its boards in Wales, with additional production in Japan
and China. These devices offer a low-cost solution and are widely used in
computer vision projects. However, newer boards like the  <a href='https://developer.nvidia.com/embedded/jetson-nano'><span class='underline'>NVIDIA Jetson Nano</span></a>
and  <a href='https://coral.ai/products/dev-board'><span class='underline'>Google Coral Dev Board</span></a> are better suited for more demanding vision
tasks.
</p> 
<p>   I have previous experience working with the <code><span class='cmtt-10'>ESP32</span></code>, which has significantly
less computational power. However, it supports offloading computations to
the cloud and can be paired with AI accelerators like the Kendryte K210.
Although the <code><span class='cmtt-10'>ESP32</span></code> is limited in the size of images it can process, it remains
effective for basic object detection. Interestingly, even microcontrollers
like the ESP32 can run lightweight models using TensorFlow Lite and
TinyML. <code><span class='cmtt-10'>AICore</span></code> is able to detect objects in 58ms, to detect objects in your
own ap you need to pair with a models such as YOLO (You Only Look
Once)<span class='footnote-mark'><a id='fn1x0-bk'><sup class='textsuperscript'>1</sup></a></span><a id='x1-2001f1'></a> that
processes an entire image in a single forward pass through a convolutional neural network
(CNN)<span class='footnote-mark'><a id='fn2x0-bk'><sup class='textsuperscript'>2</sup></a></span><a id='x1-2003f2'></a>.
It is highly efficient, making it suitable for real-time applications like robotics,
drones, and surveillance systems.
</p> 
<p>   DETR (DEtection TRansformer) introduces a novel
approach<span class='footnote-mark'><a id='fn3x0-bk'><sup class='textsuperscript'>3</sup></a></span><a id='x1-2005f3'></a> to object detection
using transformer architectures<span class='footnote-mark'><a id='fn4x0-bk'><sup class='textsuperscript'>4</sup></a></span><a id='x1-2007f4'></a>.
Unlike YOLO, which relies on anchor boxes and grid-based detection,
DETR uses a set-based global loss function, eliminating the need for
post-processing steps like non-maximum suppression. While YOLO is known
for its exceptional speed and efficiency in detecting objects in low-resource
environments<span class='footnote-mark'><a id='fn5x0-bk'><sup class='textsuperscript'>5</sup></a></span><a id='x1-2009f5'></a>,
DETR excels in handling complex scenes with overlapping objects, thanks to its attention
mechanism<span class='footnote-mark'><a id='fn6x0-bk'><sup class='textsuperscript'>6</sup></a></span><a id='x1-2011f6'></a>.
</p> 
<p>   The difference lies in their application focus:
</p> 
<p>   YOLO is optimized for edge devices and scenarios where inference speed is critical(e.g., drones
and mobile devices)<span class='footnote-mark'><a id='fn7x0-bk'><sup class='textsuperscript'>7</sup></a></span><a id='x1-2013f7'></a>.
</p> 
<p>   DETR is better suited for high-accuracy tasks where computational resources are
                                                                  

                                                                  
plentiful, such as large-scale image annotation and research workflows.
</p> 
<p>   Both models are available from repositories such as Hugging Face and
can be implemented with relatively few lines of code. In practice, a hybrid
approach—leveraging YOLO’s efficiency and DETR’s accuracy—can offer the best of
both worlds, particularly for adaptive real-time systems that balance speed and
precision. Indeed, you can train your own models using data sets such as Facebook’s
UO3D Uncommon Objects in 3D which is 19.3TB dataset features 170 thousand
videos with 1k categories of objects.
   </p> 

   <h3 class='likesectionHead' id='causality-in-drone-vision-systems'><a id='x1-3000'></a>Causality in Drone Vision Systems</h3>
<p>While object detection models like YOLO are excellent at identifying patterns—such
as detecting a person in a video frame—those patterns alone often lack the context
needed for high-precision decisions. Recognizing that a person is present is one thing,
but understanding *why* that pattern appears—what is causing it—is essential for
improving accuracy.
</p> 
<p>   For example, a person walking across a road may cause a specific motion pattern
in the scene. Detecting just the person is correlation; identifying that the person is
moving into the drone’s flight path—based on their trajectory over time—is causality.
This distinction helps reduce false positives and enables the drone to interpret
situations more precisely.
</p> 
<p>   By incorporating video transformers and temporal modeling, we begin
to move from raw detection to causal understanding—allowing drones to
process not just what is seen, but also what it means. Combined with Rust,
TensorFlow Lite, and WebAssembly, these models can be deployed on the
edge.
</p> 
<p>   Since writing this article, I have implemented WebGL using Rust on both the
server and client sides of my website <a class='url' href='https://github.com/b1ngster/bingster'><span class='cmtt-10'>https://github.com/b1ngster/bingster</span></a>.
Rust is a great option for developing WebAssembly modules thanks to its
memory safety, low-level performance, and better usability compared to
C++.
</p> 
<p>   Since the application now integrates multiple programming languages, I plan to
containerize it to streamline deployment and ensure consistency across environments.
Looking ahead, I’m exploring the use of Kubernetes for container orchestration
and considering WebAssembly to enhance performance and cross-platform
compatibility.
</p> 
<p>   <span class='footnote-mark'><a id='fn8x0-bk'><sup class='textsuperscript'>8</sup></a></span><a id='x1-3001f8'></a>.
                                                                  

                                                                  
   </p> 
<div class='footnotes'><a id='x1-2002x'></a>
<p>    <span class='footnote-mark'><a id='fn1x0'><sup class='textsuperscript'>1</sup></a></span><a class='url' href='https://pjreddie.com/darknet/yolo/'><span class='cmtt-8'>https://pjreddie.com/darknet/yolo/</span></a></p> 
<a id='x1-2004x'></a>
<p>    <span class='footnote-mark'><a id='fn2x0'><sup class='textsuperscript'>2</sup></a></span><a class='url' href='https://arxiv.org/abs/1506.02640'><span class='cmtt-8'>https://arxiv.org/abs/1506.02640</span></a></p> 
<a id='x1-2006x'></a>
<p>    <span class='footnote-mark'><a id='fn3x0'><sup class='textsuperscript'>3</sup></a></span><a class='url' href='https://ai.facebook.com/research/detr/'><span class='cmtt-8'>https://ai.facebook.com/research/detr/</span></a></p> 
<a id='x1-2008x'></a>
<p>    <span class='footnote-mark'><a id='fn4x0'><sup class='textsuperscript'>4</sup></a></span><a class='url' href='https://arxiv.org/abs/2005.12872'><span class='cmtt-8'>https://arxiv.org/abs/2005.12872</span></a></p> 
<a id='x1-2010x'></a>
<p>    <span class='footnote-mark'><a id='fn5x0'><sup class='textsuperscript'>5</sup></a></span><span class='cmr-8'>For example, YOLOv7 can achieve real-time object detection at high frame rates on devices
like the NVIDIA Jetson Nano</span></p> 
<a id='x1-2012x'></a>
<p>     <span class='footnote-mark'><a id='fn6x0'><sup class='textsuperscript'>6</sup></a></span><span class='cmr-8'>DETR’s ability to model long-range dependencies allows it to detect objects even in
cluttered environments.</span></p> 
<a id='x1-2014x'></a>
<p>     <span class='footnote-mark'><a id='fn7x0'><sup class='textsuperscript'>7</sup></a></span> <a href='https://www.edge-ai-vision.com/'><span class='cmr-8'>examples of YOLO’s deployment</span></a></p> 
<a id='x1-3002x'></a>
<p>     <span class='footnote-mark'><a id='fn8x0'><sup class='textsuperscript'>8</sup></a></span><span class='cmr-8'>For more details, see the survey at </span><a class='url' href='https://llava-vl.github.io'><span class='cmtt-8'>https://llava-vl.github.io</span></a></p> 
                 </div>
 
</body> 
</html>