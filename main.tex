\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{blindtext}

\newcommand{\click}[2]{\href{http://#1}{\colorlet{temp}{.}\color{blue}{\underline{\color{temp}#2}}\color{temp}}}  
\newcommand{\clicks}[2]{\href{https://#1}{\colorlet{temp}{.}\color{blue}{\underline{\color{temp}#2}}\color{temp}}}  

\newcommand{\cmdname}[1]{\texttt{#1}}
\newcommand\footurl[1]{\footnote{\url{#1}}}
\newcommand\urllink[2]{#1\footurl{#2}}
\newcommand\foothref[3]{#1\footnote{\href{#2}{#3}}}
\usepackage{upquote}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}
\usepackage{xcolor}
\usepackage{caption}


%\DeclareCaptionFormat{mycaption}{#1}

% Set the custom caption format for all listings
%\captionsetup[lstlisting]{format=mycaption}
\renewcommand{\lstlistingname}{}% Listing -> Algorithm
\renewcommand{\lstlistlistingname}{List of \lstlistingname s}% List of Listings -> List of Algorithms

\lstdefinestyle{csharpstyle}{
    language={[Sharp]C},
    backgroundcolor=\color{black}, % Background color of the code
    basicstyle=\color{white}\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{orange},
    numberstyle=\tiny\color{white}, % Font color of line numbers
    numbers=left,
    breaklines=true,
    showstringspaces=false,
    tabsize=4,
    frame=leftline,
    framerule=2pt,
    captionpos=t, % Position of the caption (bottom)
    numbersep=10pt, % Space between line numbers and code
    framexleftmargin=10pt, % Width of the line number background
    rulecolor=\color{black}, % Color of the line separating code and line numbers
}
\lstset{
    numbers=none, % Disable line numbering globally
}
\title{
    You Only Look Once... Unless It's Blurry \\
    \large From Blurry Pixels to Causality: Building Real-Time Vision Apps
}

\author{B1ngster}
\date{31 July 2025}

\begin{document}

\maketitle
\newpage
 
\section*{Seeing Isn’t Always Understanding}
\addcontentsline{toc}{section}{\protect\numberline{}Seeing Isn’t Always Understanding}


Ever since Donald Trump became the leader of the free world, its now become possible to achieve anything you want. Not that Donald Trump has achieved things himself - he is nepo baby who was funded by another nepo baby who becoming the head of a Government Department without being elected and he is not even American. Now, that person is no longer leading the department, and Donald Trump might no longer be president. A lot has happened in American politics since my last article, but we're not going to talk about that right now... On June 2nd, Prime Minister Keir Starmer announced that every British citizen should be prepared to go to war; perhaps we'll talk about that instead...


Anyway, I've recently felt inspired to explore augmented reality—something that's become possible now that I've been approved for a mobile contract. A friend recently asked me if I could help identify some numbers in a video file, specifically the collar number of a police officer.

I was optimistic that this would be a great opportunity to apply some data science skills. After loading the video into \cmdname{Blender}, I was able to identify most of the numbers across different frames. However, the footage was blurry, and I couldn't be certain about one of the digits.

This issue also highlights a limitation I’ve encountered with my phone. The Samsung S24’s accessibility tool, \clicks{www.samsung.com}{Bixby Vision}, offers only basic OCR—it captures text from photos, but doesn’t allow direct text selection or copying, and images often turn out partially out of focus. There are clearly more advanced apps available with richer scene understanding and better OCR accuracy.
This issue also highlights a limitation I’ve encountered with my phone. The Samsung S24’s accessibility tool, \clicks{www.samsung.com}{Bixby Vision}, offers only basic OCR—it captures text from photos, but doesn’t allow direct text selection or copying, and images often turn out partially out of focus. There are clearly more advanced apps available with richer scene understanding and better OCR accuracy.

\clicks{developers.google.com/ml-kit/overview}{Google's ML Kit} includes a Scene Semantics API capable of identifying buildings, vehicles, people, and other common objects. It also provides a document scanner API, as well as several natural language processing APIs such as entity extraction and pose detection—including depth-aware pose estimation using Z coordinates. ML Kit supports the integration of TensorFlow models, allowing developers to enhance object recognition with custom AI.



\clicks{ai.google}{Gemini} is available in ARCore, but it is currently limited to Pixel phones. Like ChatGPT, \cmdname{Gemini} is a transformer model that can understand images, though video understanding hasn’t been implemented yet. Unfortunately, many of these APIs come with usage costs, while ARCore appears to be free. It’s possible that these third-party services could see our use as potentially nefarious.

\section*{Beyond Boxes: YOLO, DETR, and the Leap Toward Causality}
\addcontentsline{toc}{section}{\protect\numberline{}Beyond Boxes: YOLO, DETR, and the Leap Toward Causality}

There are several video transformer models available—something I’m quite familiar with, as they were among the potential topics for my master’s dissertation. In the end, I chose to focus on identifying faults in the electric grid—a subject that may be worth revisiting, especially now that I’ve received a Carbon Literacy certificate.



Drones are often built using the \cmdname{Raspberry Pi}, developed by a British company that manufactures most of its boards in Wales, with additional production in Japan and China. These devices offer a low-cost solution and are widely used in computer vision projects. However, newer boards like the \clicks{developer.nvidia.com/embedded/jetson-nano}{NVIDIA Jetson Nano} and \clicks{coral.ai/products/dev-board}{Google Coral Dev Board} are better suited for more demanding vision tasks.

I have previous experience working with the \cmdname{ESP32}, which has significantly less computational power. However, it supports offloading computations to the cloud and can be paired with AI accelerators like the Kendryte K210. Although the \cmdname{ESP32} is limited in the size of images it can process, it remains effective for basic object detection. Interestingly, even microcontrollers like the ESP32 can run lightweight models using TensorFlow Lite and TinyML.
 \cmdname{AICore} is able to detect objects in 58ms, to detect objects in your own ap you need to pair with a models such as  \urllink{YOLO (You Only Look Once)}{https://pjreddie.com/darknet/yolo/} that processes an entire image in a single forward pass through a convolutional neural network (CNN)\footurl{https://arxiv.org/abs/1506.02640}. It is highly efficient, making it suitable for real-time applications like robotics, drones, and surveillance systems.

DETR (DEtection TRansformer) \urllink{introduces a novel approach}{https://ai.facebook.com/research/detr/} to object detection using transformer architectures\footurl{https://arxiv.org/abs/2005.12872}. Unlike YOLO, which relies on anchor boxes and grid-based detection, DETR uses a set-based global loss function, eliminating the need for post-processing steps like non-maximum suppression.
While YOLO is known for its exceptional speed and efficiency in detecting objects in low-resource environments\footnote{For example, YOLOv7 can achieve real-time object detection at high frame rates on devices like the NVIDIA Jetson Nano}, DETR excels in handling complex scenes with overlapping objects, thanks to its attention mechanism\footnote{DETR's ability to model long-range dependencies allows it to detect objects even in cluttered environments.}.


The difference lies in their application focus:

YOLO is optimized for edge devices and scenarios where inference speed is critical\foothref{(e.g., drones and mobile devices)}{https://www.edge-ai-vision.com/}{examples of YOLO's deployment}.

DETR is better suited for high-accuracy tasks where computational resources are plentiful, such as large-scale image annotation and research workflows.

Both models are available from repositories such as Hugging Face and can be implemented with relatively few lines of code. In practice, a hybrid approach—leveraging YOLO’s efficiency and DETR’s accuracy—can offer the best of both worlds, particularly for adaptive real-time systems that balance speed and precision.
Indeed, you can train your own models using data sets such as Facebook's UO3D Uncommon Objects in 3D which is 19.3TB dataset features 170 thousand videos with 1k categories of objects.
\section*{Causality in Drone Vision Systems}

While object detection models like YOLO are excellent at identifying patterns—such as detecting a person in a video frame—those patterns alone often lack the context needed for high-precision decisions. Recognizing that a person is present is one thing, but understanding *why* that pattern appears—what is causing it—is essential for improving accuracy.

For example, a person walking across a road may cause a specific motion pattern in the scene. Detecting just the person is correlation; identifying that the person is moving into the drone’s flight path—based on their trajectory over time—is causality. This distinction helps reduce false positives and enables the drone to interpret situations more precisely.

By incorporating video transformers and temporal modeling, we begin to move from raw detection to causal understanding—allowing drones to process not just what is seen, but also what it means. Combined with Rust, TensorFlow Lite, and WebAssembly, these models can be deployed on the edge.

Since writing this article, I have implemented WebGL using Rust on both the server and client sides of my website \url{https://github.com/b1ngster/bingster}. Rust is a great option for developing WebAssembly modules thanks to its memory safety, low-level performance, and better usability compared to C++. 

Since the application now integrates multiple programming languages, I plan to containerize it to streamline deployment and ensure consistency across environments. Looking ahead, I’m exploring the use of Kubernetes for container orchestration and considering WebAssembly to enhance performance and cross-platform compatibility.




\footnote{For more details, see the survey at \url{https://llava-vl.github.io}}.



\end{document}
